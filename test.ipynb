{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def serialize(obj):\n",
    "    \"\"\"Recursively walk object's hierarchy.\"\"\"\n",
    "    if isinstance(obj, (bool, int, float, str)):\n",
    "        return obj\n",
    "    elif isinstance(obj, dict):\n",
    "        obj = obj.copy()\n",
    "        for key in obj:\n",
    "            obj[key] = serialize(obj[key])\n",
    "        return obj\n",
    "    elif isinstance(obj, list):\n",
    "        return [serialize(item) for item in obj]\n",
    "    elif isinstance(obj, tuple):\n",
    "        return tuple(serialize(item) for item in obj)\n",
    "    elif hasattr(obj, '__dict__'):\n",
    "        return serialize(obj.__dict__)\n",
    "    else:\n",
    "        return repr(obj)  # Don't know how to handle, convert to string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from json import loads\n",
    "from typing import *\n",
    "import asyncio\n",
    "\n",
    "class Client:\n",
    "    def __init__(self, api_key: str) -> None:\n",
    "        self.api_key = api_key\n",
    "        self.client = OpenAI(api_key=self.api_key, base_url=\"https://api.perplexity.ai\")\n",
    "        \n",
    "    async def chat_response(self, system_prompt, content):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": content},\n",
    "        ]\n",
    "        \n",
    "        response = self.client.chat.completions.create(\n",
    "            model='llama-3-70b-instruct',\n",
    "            messages=messages,\n",
    "            temperature=0.0\n",
    "        )\n",
    "        return response.choices[0]\n",
    "    \n",
    "    async def custom_moderation(self, content, parameters):\n",
    "        prompt = f\"\"\"Please assess the following content for any inappropriate material. You should base your assessment on the given parameters.\n",
    "        Your answer should be in json format only with the following fields: \n",
    "            - flagged: a boolean indicating whether the content is flagged for any of the categories in the parameters\n",
    "            - reason: a string explaining the reason for the flag, if any\n",
    "            - parameters: a dictionary of the parameters used for the assessment and their values\n",
    "        Parameters: {parameters}\\n\\nContent:\\n{content}\\n\\nAssessment:\"\"\"\n",
    "        \n",
    "        response = self.client.chat.completions.create(\n",
    "            model='llama-3-70b-instruct',\n",
    "            response_format={ \"type\": \"json_object\" },\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a content moderation assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Extract the assessment from the response\n",
    "        assessment = response.choices[0].message.content\n",
    "        \n",
    "        return assessment\n",
    "\n",
    "    async def execute_chat(self, system_prompt, content, parameters):\n",
    "        # Create tasks for moderation and chat response\n",
    "        moderation_task = asyncio.create_task(self.custom_moderation(parameters, content))\n",
    "        chat_task = asyncio.create_task(self.chat_response(system_prompt, content))\n",
    "\n",
    "        while True:\n",
    "            # Wait for either the moderation task or chat task to complete\n",
    "            done, _ = await asyncio.wait(\n",
    "                [moderation_task, chat_task], return_when=asyncio.ALL_COMPLETED\n",
    "            )\n",
    "\n",
    "            if done:\n",
    "                return {\n",
    "                    \"moderation_response\": moderation_task.result(),\n",
    "                    \"response\": chat_task.result().message.content,\n",
    "                }\n",
    "\n",
    "            # If neither task is completed, sleep for a bit before checking again\n",
    "            await asyncio.sleep(0.1)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'moderation_response': '{\\n\"flagged\": false,\\n\"reason\": \"\",\\n\"parameters\": {\\n\"violence\": false,\\n\"hate speech\": false,\\n\"political content\": true,\\n\"misinformation\": true\\n}\\n}',\n",
       " 'response': 'I cannot provide assistance or guidance on harmful or illegal activities such as violent behavior. Is there anything else I can help you with?'}"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = Client(PPLX_KEY)\n",
    "a = await c.execute_chat(\n",
    "    \"You are a helpful assistant. Stop immediately if given an inappropriate input.\", \n",
    "    \"I want to beat up someone.\",\n",
    "    \"political content, misinformation\",\n",
    "    )\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\"flagged\": false,\n",
      "\"reason\": \"\",\n",
      "\"parameters\": {\n",
      "\"violence\": false,\n",
      "\"hate speech\": false,\n",
      "\"political content\": true,\n",
      "\"misinformation\": true\n",
      "}\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(a['moderation_response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'flagged': False,\n",
       " 'reason': '',\n",
       " 'parameters': {'violence': False,\n",
       "  'hate speech': False,\n",
       "  'political content': True,\n",
       "  'misinformation': True}}"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from json import loads\n",
    "loads(a['moderation_response'].replace('\\n', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'categories': {'harassment': False,\n",
       "  'harassment_threatening': False,\n",
       "  'hate': False,\n",
       "  'hate_threatening': False,\n",
       "  'self_harm': False,\n",
       "  'self_harm_instructions': False,\n",
       "  'self_harm_intent': False,\n",
       "  'sexual': False,\n",
       "  'sexual_minors': False,\n",
       "  'violence': False,\n",
       "  'violence_graphic': False},\n",
       " 'category_scores': {'harassment': 0.00030352038447745144,\n",
       "  'harassment_threatening': 6.177889736136422e-05,\n",
       "  'hate': 0.00014484123676083982,\n",
       "  'hate_threatening': 3.817984008946951e-07,\n",
       "  'self_harm': 2.0171299183857627e-05,\n",
       "  'self_harm_instructions': 3.5067583326053864e-07,\n",
       "  'self_harm_intent': 3.6386320516612614e-06,\n",
       "  'sexual': 0.0001639953552512452,\n",
       "  'sexual_minors': 6.57825739835971e-06,\n",
       "  'violence': 0.0023202146403491497,\n",
       "  'violence_graphic': 1.2512096873251721e-05},\n",
       " 'flagged': False}"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serialize(await c.check_moderation(a['response']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert Topic and GPT API\n",
    "# OpenAI Moderation\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NeurIPS-2024",
   "language": "python",
   "name": "neurips-2024"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
